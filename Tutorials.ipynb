{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorials.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rGqst8aK6UfT",
        "cSnqdvUFCCjr",
        "3kJY9EiW7_RC",
        "iZO_XnbV9nEz",
        "BxepbcqcCxN5",
        "ydeBpTwdIWfk",
        "UulnF241hCdn",
        "kCpydd-Cf2bk",
        "0tXW7CCuCKzT",
        "YOvVfw8OqGEB",
        "RwbbuLKk1eIM"
      ],
      "authorship_tag": "ABX9TyOhsFZ+Uh/Ifp4wtNp1Z1qG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scrameri/Umweltbiologie/blob/master/Tutorials.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSnqdvUFCCjr",
        "colab_type": "text"
      },
      "source": [
        "## Install packages\n",
        "This Jupyter notebook is based on the programming language [`R`](https://www.r-project.org/about.html) and has been tested on `R` version 3.6.3.\n",
        "\n",
        "Author: [Simon Crameri](https://peg.ethz.ch/people/person-detail.MTYzNTY1.TGlzdC80MzUsLTczMzM3Mzg4Ng==.html), ETH Zurich, May 2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAtH7B0C6PrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "need.pckg <- c(\"GGally\",\"ggplot2\",\"EnvStats\",\"multcomp\",\"factoextra\") # needed for this script\n",
        "if (any(!need.pckg %in% installed.packages())) {\n",
        "  for (i in need.pckg[!need.pckg %in% installed.packages()]) {\n",
        "    cat(\"installing\", i, \"...\\n\")\n",
        "    install.packages(i)\n",
        "  }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kJY9EiW7_RC",
        "colab_type": "text"
      },
      "source": [
        "### Load libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2OL_NAN8HLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "library(\"GGally\") # for ggpairs()\n",
        "library(\"ggplot2\") # for ggplot()\n",
        "library(\"EnvStats\") # for qqPlot()\n",
        "library(\"multcomp\") # for glht()\n",
        "library(\"factoextra\") # for fviz_pca() plots"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGqst8aK6UfT",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "## Data Exploration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L549L6B8fZO",
        "colab_type": "text"
      },
      "source": [
        "### Read the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0H-kKML8jD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "morph <- read.csv(\"https://raw.githubusercontent.com/scrameri/Umweltbiologie/master/Puzzle_ANOVA_LM.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih8AwVxs9ML4",
        "colab_type": "text"
      },
      "source": [
        "NOTE: the .csv file uses \",\" rather than \";\" as field separators, which can cause troubles when opening the file in Excel on Mac. You can also save a version separated with \";\", but you will then need to set `sep=\";\"`, because the default argument is `read.csv(file, sep = ',')`. Also have a look at the `read.csv2` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZO_XnbV9nEz",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "### Get an overview over the dataset\n",
        "This dataset contains morphological measurements of eight populations of [*Dianthus carthusianorum*](https://www.infoflora.ch/de/flora/dianthus-carthusianorum.html) taken in Summer 2015 in Wallis (Switzerland). Populations were located in two classes of elevation (4 high, 4 low elevation).\n",
        "\n",
        "A few facts about [*D. carthusianorum*](https://de.wikipedia.org/wiki/Kart%C3%A4usernelke)\n",
        "- KarthÃ¤user-Nelke in German, family Caryophyllaceae like *Silene*\n",
        "- native to Middle Europe, in dry habitats from colline to alpine (introduced to N America) \n",
        "- [gynodioecious](https://en.wikipedia.org/wiki/Gynodioecy), meaning that there are female and hermaphroditic individuals\n",
        "- perennial (or biennial), meaning that plants survive winter as rosettes or seeds\n",
        "- insect-pollinated\n",
        "- parasite: fungus [*Microbothryum*](https://fr.wikipedia.org/wiki/Microbotryum) *dianthorum* (anther smut fungus) that causes sterility and is transmitted by pollinators\n",
        "\n",
        "A few facts about the dataset\n",
        "- ID: individual ID\n",
        "- Population: population ID\n",
        "- Elevation: elevation class\n",
        "- Date: measurement date\n",
        "- Infection: whether a plant was [infected](https://upload.wikimedia.org/wikipedia/commons/e/ef/Microbotryum_dianthorum_Dianthus_sp._2019_07_18_05.jpg) or healthy\n",
        "- Sex: whether a plant was hermaphroditic or female\n",
        "- Stalk_height: mean stalk height per plant (mm)\n",
        "- Stalk_count: number of stalks per plant\n",
        "- Bud_count: number of buds per plant\n",
        "- Flower_count: number of open flowers per plant\n",
        "- Flower_diam: flower (corolla) diameter (mm)\n",
        "- Petal and Sepal lengths and widths (mm)\n",
        "- Rosette_diam1: diameter of first rosette (mm) - only recorded at high elevation\n",
        "- Rosette_diam2: diameter of second rosette (mm) - only recorded at high elevation\n",
        "\n",
        "The experimental design and measurements have been done by [Ursina Walther](https://peg.ethz.ch/people/person-detail.html?persid=158239) (a PhD student in our group). She was interested in studying the evolution of floral traits in this species, especially in relation to the interaction between the plants and their Microbothryum parasite."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D29eD9yZXbS",
        "colab_type": "text"
      },
      "source": [
        "Before starting any analysis, always check that the data was read in *correctly*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAhwlGZ49Nqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "str(morph) # lists all variables in the data.frame and their classes\n",
        "ncol(morph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoEl6ccQBnB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dim(morph) # gives both nrow() and ncol() of the dataset\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0Y9LPAJBoZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summary(morph$Stalk_height) # gives a summary for one variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akt6WCz2CQZo",
        "colab_type": "text"
      },
      "source": [
        "![Question](https://img.icons8.com/flat_round/64/000000/question-mark.png)\n",
        " Based on what you now know about the study and the data:\n",
        "- what are the response variable(s)?\n",
        "- what are the explanatory variable(s)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxepbcqcCxN5",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "\n",
        "### Plot the data\n",
        "There are various plotting techniques to visualize data. Some people prefer the `R` base graphics, others learn the grammar of graphics implemented in [`ggplot2`](https://ggplot2.tidyverse.org/). For this practical, both approaches can be used. These are the basic functions for the most important plots, shown for `R` base graphics and `ggplot2` graphics:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR84dKhii6o-",
        "colab_type": "text"
      },
      "source": [
        "Use **Scatterplots** to plot two numeric variables (classes `num` for continuous, or `int` for discrete data) against each other. Scatterplots can be more informative if different *symbols* or *colours* for an additional factor, or *sizes* or *transparency* for an additional numeric variable are used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTKf9L38C3r4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# base\n",
        "plot(Sepal_length ~ Petal_length, data = morph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaXTNlvIC6pI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ggplot2\n",
        "ggplot(data = morph, aes(x = Petal_length, y = Sepal_length)) + geom_point()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8swjuZPgDrMx",
        "colab_type": "text"
      },
      "source": [
        "Use **Boxplots** to plot a numeric variable against a categorical variable (class `factor`). Boxplots are summarizing the data. By default, the box is the *interquartile range* and contains 50% of the data points, while the whiskers extend to \"the most extreme data point which is no more than 1.5 times the interquartile range from the box\" (see cell 85). If you have less than eight data points per factor level, you might just aswell plot all the data points using `stripchart(Sepal_length ~ Elevation, data = morph, vertical = TRUE)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSQQvQpBDtai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# base\n",
        "boxplot(Sepal_length ~ Elevation, data = morph, ylim = c(8,20))\n",
        "\n",
        "# understand interquartile range (iqr): range from 25% to 75% quantiles, contains 50% of the data\n",
        "iqr1 <- quantile(morph$Sepal_length[morph$Elevation==\"high\"], c(0.25,0.75))\n",
        "iqr2 <- quantile(morph$Sepal_length[morph$Elevation==\"low\"], c(0.25,0.75))\n",
        "\n",
        "# whiskers extend to the most extreme data point not exceeding 1.5*iqr from the box (by default)\n",
        "whisker1 <- iqr1 + c(-1.5,1.5)*diff(iqr1)\n",
        "whisker2 <- iqr2 + c(-1.5,1.5)*diff(iqr2)\n",
        "\n",
        "segments(0.5, whisker1, 1.5, whisker1, col = \"tomato\")\n",
        "segments(1.5, whisker2, 2.5, whisker2, col = \"blue\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA8ZBSWBD26I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ggplot2\n",
        "ggplot(data = morph, aes(x = Elevation, y = Sepal_length)) + geom_boxplot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KacbjSFqHP23",
        "colab_type": "text"
      },
      "source": [
        "Use **Histograms** to plot a variable's distribution. The `breaks` argument in `hist` and the `bins` argument in `geom_histogram` can be used to fine-tune the binning of values into histogram categories. Try and play around with it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmXzuh5IH0qk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist(morph$Sepal_length, breaks = 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUipUhQqH79a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ggplot(data = morph, aes(x = Sepal_length)) + geom_histogram(bins = 20) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCGAbF74HtBt",
        "colab_type": "text"
      },
      "source": [
        "Use **Barplots** to plot all values of a single variable or a table of counts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZUC4O_IHQ-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# base\n",
        "barplot(table(morph$Population))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odALebdKHdnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ggplot2 (uses count table by default!)\n",
        "ggplot(data = morph, aes(x = Population)) + geom_bar(stat = \"count\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC7XnRTvIIhv",
        "colab_type": "text"
      },
      "source": [
        "Use **Mosaic Plots** or **Stacked Barplots** to plot *contingency tables* of two categorical variables against each other. In a Mosaic Plot, the relative size of each mosaic is proportional to the number of observations with a given factor level combination. The stacked barplot is slightly different, once shown as stacked *counts* and once as stacked *proportions*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K252kD91IJRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# base\n",
        "mosaicplot(table(morph$Elevation,morph$Infection), main = \"Mosaic Plot\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK54oZskIL5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ggplot2 (fill variable determines the stacking)\n",
        "ggplot(morph, aes(x = Elevation, fill = Infection)) + \n",
        "  geom_bar() + \n",
        "  ggtitle(\"Stacked Barplot\")\n",
        "  \n",
        "ggplot(morph, aes(x = Elevation, fill = Infection)) + \n",
        "  geom_bar(position = \"fill\") + labs(y = \"Proportion\") +\n",
        "  ggtitle(\"Stacked Barplot\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjTx6lDvLSGY",
        "colab_type": "text"
      },
      "source": [
        "![Question](https://img.icons8.com/flat_round/64/000000/question-mark.png)\n",
        "- Can you produce a plot of `Petal_length ~ Sepal_length` with points color-coded for `Infection` and size proportional to `Petal_length`?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VibMsUDaXb7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make your plot here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydeBpTwdIWfk",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "### Generate all pairwise plots\n",
        "#### Use regular expressions\n",
        "Given that we have 17 variables, plotting all of them against each other by hand would be tedious. For datasets with a moderate number of variables, you can use the `ggpairs` function from the [*GGally*](https://cran.r-project.org/web/packages/GGally/index.html) package to get a graphical overview over many variables at once with a single line of code. The function produces all possible pairwise plots, minimizes redundancy and takes care of factors and numerical variables automatically.\n",
        "\n",
        "With 17 variables, plotting all against all would lead to too many (289) plots on a single page. Let us therefore subset the variables. You can use `grep` and [**regular expressions**](https://regex101.com/) to find the indices of certain variable names, this often saves code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bLAptJ2J-dM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this returns the index of variable names *starting* with 'Flower' (^ specifies the *start*)\n",
        "grep(pattern = \"^Flower_\", x = names(morph))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10NHUc4_KBAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this returns the index of variable names *ending* with either 'length' or 'width' ($ specifies the *end*, | is a logial <or>)\n",
        "grep(pattern = \"length$|width$\", x = names(morph)) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4zfZ7GOgKyA",
        "colab_type": "text"
      },
      "source": [
        "![Question](https://img.icons8.com/flat_round/64/000000/question-mark.png)\n",
        "- Can you identify the indices of columns with variables of *fertile* structures (buds, flowers) using a regular expression?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVsEmbd4gklv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create your regular expression here\n",
        "names(morph)\n",
        "grep(pattern = \"XXX\", x = names(morph))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UulnF241hCdn",
        "colab_type": "text"
      },
      "source": [
        "#### print all pairwise plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UisrQsj_KHP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(vars.fertile <- names(morph)[grep(pattern = \"^Flower|^Bud|^Petal|^Sepal\", x = names(morph))])\n",
        "print(vars.sterile <- names(morph)[grep(pattern = \"^Stalk|^Rosette\", x = names(morph))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYt4oOl7KPDa",
        "colab_type": "text"
      },
      "source": [
        "The `ggpairs` function prepares the pairwise plots as `gg` class objects. These can then be printed to the plotting device, or saved to a PDF file by enclosing the print commands between the `pdf` and `graphics.off` functions.\n",
        "\n",
        "It is more reproducible (and therefore more scientific) to save plots with such code rather than by manually exporting a plot from R Studio. The specified `height` and `width` also prevents the saved plot to be smaller or larger depending on your device size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBic2sG3KNG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pairs.fertile <- ggpairs(data = morph, columns = c(\"Population\",\"Elevation\",\"Infection\",\"Sex\", vars.fertile))\n",
        "pairs.sterile <- ggpairs(data = morph, columns = c(\"Population\",\"Elevation\",\"Infection\",\"Sex\", vars.sterile))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CV-vm_cKXGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pdf(\"Pairsplots.pdf\", height = 15, width = 15)\n",
        "#pairs.sterile # or print(pairs.sterile)\n",
        "pairs.fertile # or print(pairs.fertile)\n",
        "#graphics.off()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxRBftCPfY5n",
        "colab_type": "text"
      },
      "source": [
        "Does everything look all right?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCpydd-Cf2bk",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "### Transparent data fixes\n",
        "If there are obvious data errors, do not manually change the original data. Rather think carefully what the issue might be, and if you can be sure enough, make the data changes in the analysis script, such that fixes become **transparent** and **reproducible**. If you cannot reconstruct what happened with an impossible data value, you should set it to `NA` or remove the entire data row in the analysis script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW5vD0FGfr08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "morph$Flower_diam[morph$Flower_diam > 100] # Dianthus with 185 mm flower diameter is impossible\n",
        "morph$Flower_diam[morph$Flower_diam > 100] <- morph$Flower_diam[morph$Flower_diam > 100]/10 # if you are sure it is a comma-error\n",
        "# morph$Flower_diamesser[morph$Flower_diamesser > 100] <- NA # if you cannot be sure what happened\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHahzxpoLipu",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "\n",
        "### Check the distribution of your variables\n",
        "Knowing the distribution of your variables is important, especially if you want to create models of them that make certain assumptions on distributions. You have already seen that the **histogram** is a good visualization of a variable's distribution because deviations from a normal distribution become intuitively apparent.\n",
        "\n",
        "Let us look at `Stalk_count`, which is a count variable and therefore by definition bound by zero on the left, and unbounded on the right. Such *right-skewed* distributions are not normally distributed.\n",
        "\n",
        "But which statistical distribution is best at describing your data if not a normal distribution? Knowing this would be helpful, e.g. to define an appropriate error distribution in a generalized linear model. **Simulations** can help you to compare your data against data from a specific statistical distribution. The `R` package [*EnvStats*](https://cran.r-project.org/web/packages/EnvStats/index.html) allows for simulation of different probability distributions from your data, and comparison of these against your data in so-called **Q-Q Plots** (Quantile-Quantile plots)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JxxPVk-L4yr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# see ?EnvStats::Distribution.df for a list comprehensive list of continuous and discrete distributions\n",
        "par(mfrow=c(2,3))\n",
        "hist(morph$Stalk_count, breaks = 20, col = \"tomato\")\n",
        "qqPlot(morph$Stalk_count, distribution = \"pois\", estimate.params = TRUE, add.line = TRUE, points.col = \"tomato\")\n",
        "qqPlot(morph$Stalk_count, distribution = \"geom\", estimate.params = TRUE, add.line = TRUE, points.col = \"tomato\")\n",
        "hist(morph$Flower_diam, breaks = 20, col = \"cadetblue\")\n",
        "qqPlot(morph$Flower_diam, distribution = \"norm\", estimate.params = TRUE, add.line = TRUE, points.col = \"cadetblue\")\n",
        "qqPlot(morph$Flower_diam, distribution = \"exp\", estimate.params = TRUE, add.line = TRUE, points.col = \"cadetblue\")\n",
        "par(mfrow=c(1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxLLE2gZMzpb",
        "colab_type": "text"
      },
      "source": [
        "![Question](https://img.icons8.com/flat_round/64/000000/question-mark.png)\n",
        "- Which probability distribution is best at describing `Stalk_count`? \n",
        "- Which probability distribution is best at describing `Flower_diam`? \n",
        "- What does this mean for an ANOVA model `Stalk_count ~ Elevation` fitted to this data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "im5uQTETmzzB",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "\n",
        "## Analysis of Variance and Linear Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tXW7CCuCKzT",
        "colab_type": "text"
      },
      "source": [
        "### The relationship between t-test, ANOVA and linear models\n",
        "\n",
        "**Analysis of Variance (ANOVA)** using `aov` and (generalized) **linear models** using `lm`or `glm` are closely related to each other and differ mainly in *intent* of analysis and default *presentation* of results. If you read the aov help page, you will see that `aov` actually calls `lm`, so both rely on the same [least squares](https://en.wikipedia.org/wiki/Least_squares) method.\n",
        "\n",
        "\n",
        "You can think of ANOVA and linear models as extensions of a two-sample [**t-test**](https://en.wikipedia.org/wiki/Student%27s_t-test) that allow for tests of more factors or covariates at once. Remember that the two-sample t-test allows to test the difference in a dependent variable (a.k.a. **response** variable, measure variable, Y variable) between two levels of one factor (a.k.a. **predictor** variable, independent variable, X variable). An example two-sample t-test is the test for difference in human height (the response variable) between two sexes (the predictor variable). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb5oHxYDm4nW",
        "colab_type": "text"
      },
      "source": [
        "Let us start with a **one-sample t-test**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRusYfHPnAzG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t.test(morph$Sepal_length, mu = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDEOIZMWnJsk",
        "colab_type": "text"
      },
      "source": [
        "![Question](https://img.icons8.com/flat_round/64/000000/question-mark.png)\n",
        "- What are the null and alternative hypotheses? \n",
        "- What is the test result?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Azjxj5wfndAa",
        "colab_type": "text"
      },
      "source": [
        "This test can also be accomplished using `lm`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjuwN66znlnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summary(lm(Sepal_length ~ 1, data = morph))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y3a2VMgnoZx",
        "colab_type": "text"
      },
      "source": [
        "Let us now extend this to a **two-sample t-test** (one sample at low and one at high elevation):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG_DHh2Knqkg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# complicated syntax, showing the two samples (sepal lengths at high and low elevation)\n",
        "# t.test(morph$Sepal_length[morph$Elevation == \"high\"], morph$Sepal_length[morph$Elevation == \"low\"], data = morph, mu = 0)\n",
        "\n",
        "# shorter and more elegant formula notation \n",
        "t.test(Sepal_length ~ Elevation, data = morph, mu = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92xQF2GSmt2w",
        "colab_type": "text"
      },
      "source": [
        "Again, the same test can be accomplished using `lm` or `aov`. A `summary` on `aov` will give you the **ANOVA table**, while `lm` will give you **effect size** estimates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTIj3zosoDv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat(\"**ANOVA table**\\n\")\n",
        "summary(aov(Sepal_length ~ Elevation, data = morph))\n",
        "\n",
        "cat(\"\\n\\n**Effect size estimates**\\n\")\n",
        "summary(lm(Sepal_length ~ Elevation, data = morph))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snB7wzYmoIGT",
        "colab_type": "text"
      },
      "source": [
        "![Question](https://img.icons8.com/flat_round/64/000000/question-mark.png)\n",
        "- What are the null and alternative hypotheses? \n",
        "- What is the estimated effect size of Elevation?\n",
        "- How do you interpret the Elevation effect estimate intuitively?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOvVfw8OqGEB",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "\n",
        "### One-way ANOVA and simple linear models\n",
        "\n",
        "The **one-way ANOVA** allows for comparing more then just two groups (i.e. one treatment factor can have more than two levels). \n",
        "\n",
        "Tests with more than 2 levels in a predictor variable can have different intents: \n",
        "\n",
        "1. test for an *overall* effect of a factor such as `Population` on calyx length\n",
        "2. test for an effect of a *specific* factor level, for instance a treatment relative to a *control*, including effect size estimates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEBKLgr3qYne",
        "colab_type": "text"
      },
      "source": [
        "If your goal is 1., then you can use `aov`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi-RX1W6qZt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summary(aov1 <- aov(Sepal_length ~ Population, data = morph))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkMEfbbQqgQV",
        "colab_type": "text"
      },
      "source": [
        "If your goal is 2., then you can use `lm`. Note that a `summary` on `lm` does not give you an answer to the hypothesis \"Population has an effect on Sepal_length\", while a `summary` on `aov` does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj2epHoCqgYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summary(lm1 <- lm(Sepal_length ~ Population, data = morph))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74lfbRiQ8_NT",
        "colab_type": "text"
      },
      "source": [
        "Irrespective of whether you fitted a model with `aov` or `lm`, you can use some handy accessor functions to **extract key model results**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVRoZQhq-Z4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## model parameter estimates in coef()\n",
        "print(coef(lm1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHJYDwkw9J4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## confidence interval (by default 95%) for model parameter estimates in confint()\n",
        "print(confint(aov1,  level = 0.95))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkUKh8H39U6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tukey-Anscombe plot (see paragraph on Residual Analysis)\n",
        "dfit <- data.frame(resid = resid(lm1), fitted = fitted(lm1))\n",
        "ggplot(dfit, aes(fitted, resid)) +\n",
        "  geom_point() +\n",
        "  geom_smooth(method = \"loess\", formula = y ~ x) +\n",
        "  theme_bw()\n",
        "\n",
        "# Observed values in black, fitted values in red\n",
        "ggplot(morph, aes(Population, Sepal_length)) + \n",
        "  geom_boxplot() + \n",
        "  geom_boxplot(aes(Population, fitted(lm1)), col = \"tomato\", linetype = 2) +\n",
        "  theme_bw()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_UG9h5H-qLR",
        "colab_type": "text"
      },
      "source": [
        "![Question](https://img.icons8.com/flat_round/64/000000/question-mark.png)\n",
        "- Why are there only 7 estimates of population model parameters instead of 8?\n",
        "- Can you tell from the **confidence intervals** which population(s) are significantly different from the reference population?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwbbuLKk1eIM",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "\n",
        "### Multi-way ANOVA and multiple linear regression\n",
        "There are many extensions to one-way ANOVA and simple linear models. In `R`, you can use the same functions `aov` and `lm` (or `glm`) and provide them with a more complex model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KixqbpOujY9L",
        "colab_type": "text"
      },
      "source": [
        "The **Factorial ANOVA** extends the one-way ANOVA to test >1 predictors (each with two or more levels), including interactions.\n",
        "\n",
        "The **mathematical model** for two predictors without interaction is as follows:\n",
        "* Y = Î²<sub>0</sub> + Î²<sub>1</sub>\\*X<sub>1</sub> + Î²<sub>2</sub>\\*X<sub>2</sub> + E ; E ~ norm(0, sd) i.i.d.\n",
        "\n",
        "Y, X<sub>1</sub>, and X<sub>2</sub> are given. The model fitting consists of estimating distributions for Î²<sub>0</sub>, Î²<sub>1</sub> and Î²<sub>2</sub>\n",
        "while minimizing the error term E (approximated by the so-called residuals).\n",
        "\n",
        "The corresponding **model formula** in `R` is as follows (no need to specify intercept and error term):\n",
        "\n",
        "* `Y ~ X1 + X2`\n",
        "\n",
        "Three preditors, the second nested in the first:\n",
        "* `Y ~ X1/X2 + X3`\n",
        "\n",
        "Three predictors, two of them interacting:\n",
        "* `Y ~ X1 + X2 + X1:X2 + X3`\n",
        "* `Y ~ (X1 + X2)^2 + X3`\n",
        "\n",
        "Three predictors, include all two-way interactions:\n",
        "* `Y ~ X1 + X2 + X1:X2 + X1:X3 + X2:X3`\n",
        "* `Y ~ (X1 + X2 + X3)^2`\n",
        "\n",
        "Three predictors, include all three-way interactions (rarely used):\n",
        "* `Y ~ (X1 + X2 + X3)^3`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqLUgQ7SXbBp",
        "colab_type": "text"
      },
      "source": [
        "Beware of the use of highly **correlated predictor variables**. If such are present, the inference can be quite wrong when both are included in the model, as shown in the following example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj27lAAbXsVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# simulate dataset with highly correlated x1 and x2 (both correlated with y)\n",
        "x1 <- rnorm(n = 500, mean = 0, sd = 1)\n",
        "x2 <- pi + pi*x1 + rnorm(n = 500, mean = 0, sd = 0.5)\n",
        "y  <- x2 + rnorm(n = 500, mean = 0, sd = 2)\n",
        "\n",
        "dsim <- data.frame(y = y, x1 = x1, x2 = x2)\n",
        "\n",
        "# correlation among x1 and x2\n",
        "cat(\"correlation between x1 and x2\\n\")\n",
        "print(cor(dsim$x1, dsim$x2))\n",
        "\n",
        "# model summaries\n",
        "cat(\"\\n\\n=>model with only x1 included\")\n",
        "summary(lm(y ~ x1, data = dsim))\n",
        "\n",
        "cat(\"\\n\\n=>nmodel with only x2 included\")\n",
        "summary(lm(y ~ x2, data = dsim))\n",
        "\n",
        "cat(\"\\n\\n=> model with x1 and x2 included (both highly correlated)\")\n",
        "summary(lm(y ~ x1 + x2, data = dsim))\n",
        "\n",
        "# graph\n",
        "ggplot(dsim, aes(x1, y, size = x2)) + geom_point() + theme_bw()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxGaW1OnequB",
        "colab_type": "text"
      },
      "source": [
        "![Question](https://img.icons8.com/flat_round/64/000000/question-mark.png)\n",
        "- What can you do to prevent the above problem?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF3Byeb0v3LY",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "\n",
        "### ANCOVA and GLMs\n",
        "\n",
        "The **ANCOVA** (Analysis of Covariance) allows for the mixed use of numerical and categorical predictors. **Generalized linear models** (GLMs) extend ANCOVA by allowing for non-normal error distribution (logistic regression, poisson regression, ...):\n",
        "\n",
        "Logistic regression\n",
        "* E ~ Binom(k|p,n) i.i.d\n",
        "\n",
        "Poisson regression\n",
        "* E ~ Poisson(lambda) i.i.d."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJdONlwJ2ksx",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "### Interactions\n",
        "Interactions between predictor (explanatory) variables can be present, and including them in a model can *greatly* improve the model fit and hence the reliability of test results.\n",
        "\n",
        "An **interaction** occurs if the effect of variable A on the response Y *depends* on variable B.\n",
        "\n",
        "Example:\n",
        "You want to model reaction time as Y ~ A + B\n",
        "\n",
        "* Y = time needed to start breaking after an obstacle appears on the street\n",
        "* A = amount of coffee drunk \n",
        "* B = amount of whisky drunk \n",
        "\n",
        "A has a negative effect on reaction time (i.e., more coffee, less reaction time), while B has a positive effect on reaction time (i.e, more whisky, more reaction time).\n",
        "\n",
        "You can increase coffee consumption to shorten reaction time, but this effect will depend on the amount of whisky drunk. That is, the amount of coffee *interacts* with the amount of whisky."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlV_2xrz3O-C",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "`R` implements some methods to visualize potential interactions. The idea is to visualize the mean (or other summary) value of the `response` Y at different levels of an explanatory variable of interest (`x.factor`), depending on the level of another and potentially interacting explanatory factor (`trace.factor`).\n",
        "\n",
        "![Question](https://img.icons8.com/flat_round/64/000000/question-mark.png)\n",
        "We now want to test the effect of `Infection` and `Altitude` on `Petal_length`. Before you fit any model, make sure to plot the data to get a better feeling for it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH9Kff6D3Ju1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Make a clever plot(s) that shows Petal_legnth, Infection and Altitude"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxHR7hdkwBKX",
        "colab_type": "text"
      },
      "source": [
        "`Petal_length` appears to be lower in some infected individuals, but that does not seem to be the case in low `Elevation` habitats. That is, the effect of `Infection` on `Petal_length` appears to *depend* on `Elevation`. Or in other words: `Infection` and `Elevation` likely *interact*.\n",
        "\n",
        "The following plot visualizes potential interactions. If the lines cross, the factors strongly interact and you would need to test for the interaction term in any model of `Petal_Length`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw5f4fkm4p_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "interaction.plot(x.factor = morph$Infection, trace.factor = morph$Elevation, response = morph$Petal_length, \n",
        "                 fun = mean, xlab = \"Infection\", ylab = \"Mean sepal length\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZi5RXgh50_S",
        "colab_type": "text"
      },
      "source": [
        "![Question](https://img.icons8.com/flat_round/64/000000/question-mark.png)\n",
        "We now want to formally test for the effect of `Infection` and `Elevation` (including their potential interaction) on `Petal_length`, and estimate the effect sizes.\n",
        "* which function do you use (`aov` or `lm`)?\n",
        "* which formula do you use?\n",
        "* what are the test results?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVmgKGrD6gJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Create your model here\n",
        "summary(mod1 <- lm(Petal_length ~ Infection * Elevation, data = morph))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-X0N7S3v4LJ",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "\n",
        "### Check model assumptions via Residual Analysis\n",
        "Up to now, we have just blindly executed `summary` on `aov` and `lm` fits and interpreted the test results. In practice, this is **not sufficient**.\n",
        "\n",
        "In order to validate the model, we need to check that the model assumptions are met, or at least not grossly violated.\n",
        "\n",
        "ANOVA and linear regression models both make the following main **assumptions**:\n",
        "  \n",
        "* [**Normality:**](https://en.wikipedia.org/wiki/Normal_distribution) For any fixed value of X, Y is *normally distributed*. This implies that the *residuals* are normally distributed. However, it does not mean that all variables have to be normally distributed (not even within groups).\n",
        "* [**Homoscedasticity:**](https://en.wikipedia.org/wiki/Homoscedasticity) The *residual variance* is *constant* (the same for any value of X)\n",
        "* [**Independence:**](https://en.wikipedia.org/wiki/Independence_(probability_theory)) Observations are *independent* of each other (i.e. one measurement is not influenced by another)\n",
        "\n",
        "If one or more of these assumptions (see this [link](http://r-statistics.co/Assumptions-of-Linear-Regression.html) for more details) are **violated**, you **cannot have trust in your test results** and in any conclusions drawn from them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec28gVm-88sL",
        "colab_type": "text"
      },
      "source": [
        "You can check most assumptions by applying the generic `plot` function on your model fit. Since this will give you **four diagnostic plots**, it is best to divide the plotting device into four panels using `par`.\n",
        "\n",
        "The first plot (**Tukey-Anscombe Plot**) is good to check for the assumption of *constant variance* of the residuals. This assumption is violated if there is a funnel-shape ur U-shaped distribution.\n",
        "\n",
        "The second plot (**Normal Q-Q Plot**) is good to check for the assumption of *normality* of the residuals. If the points follow the diagonal, they follow a normal distribution. If you don't trust your eye, you can also formally test the null hypothesis of normality using a **Shapiro-Wilk normality test** using `shapiro.test` on the model residuals. If that test turns out significant (p < 0.05), it means that the null hypothesis of normality is rejected, and hence that the residuals are not normally distributed.\n",
        "\n",
        "The **Scale-Location Plot** is similar to the Tukey-Anscombe Plot but plots the square-root of absolute residuals, which allows to see other aspects of residual variance. \n",
        "\n",
        "The **Leverage Plot** is good to identify outliers (points that lie outside of Cook's distance). Such outliers could have a strong effect on the model fit and test results and should be investigated further if present.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8fwQw9t9OOR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Residual analysis produces 4 plots by default.\n",
        "par(mfrow = c(2,2)) #Â prepares an plot device layout with 2 columns and 2 rows (4 plots)\n",
        "plot(mod1)\n",
        "par(mfrow = c(1,1))\n",
        "\n",
        "shapiro.test(mod1$resid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDh-k8PDEh7M",
        "colab_type": "text"
      },
      "source": [
        "Independence of measurements can be assessed by plotting a response variable's values in their order of measurements, or with a **Serial Correlation Plot** using the `acf` function (assuming the the order of measurements of `Petal_length` in the table corresponds to the order of measurements taken on each `Date`. If the autocorrelation function ACF is within the blue dotted lines for lags > 0, the measurements are not significantly autocorrelated (or dependent)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t94xeucYEoN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ggplot(morph, aes(seq_along(Sepal_length), Sepal_length)) + \n",
        "  geom_line() + geom_point(aes(color = Infection, shape = Infection)) +\n",
        "  labs(x = \"Order of measurement\") +\n",
        "  facet_wrap(~ Elevation + Date + Population, scales = \"free_x\") + \n",
        "  theme_bw()\n",
        "\n",
        "acf(resid(mod1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebw_wot09ine",
        "colab_type": "text"
      },
      "source": [
        "![Question](https://img.icons8.com/flat_round/64/000000/question-mark.png)\n",
        "* Are the assumptions of normality, constant variance and independence of the residuals met?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv8H-7E98R0j",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "### Variable transformations\n",
        "Apart from interaction effects, transformations of the resonse Y or the numerical predictors X often help to improve model fit, at the expense of interpretability of the fitted model parameters (estimated effect sizes). There are the following commonly used **first-aid transformations**.\n",
        "\n",
        "- for *positive counts*:               `X <- sqrt(X)`\n",
        "- for *positive continuous variables*: `X <- log(X)`\n",
        "- for *fractions* [0,1]:               `X <- asin(sqr(X))`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aztFqSbKAtMn",
        "colab_type": "text"
      },
      "source": [
        "![Question](https://img.icons8.com/flat_round/64/000000/question-mark.png)\n",
        "Now try to apply a transformation to the response.\n",
        "* Does the model fit improve?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNoWD04IJ0kn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Create your model here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU9jro8IAyeH",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "### Compare nested models\n",
        "Complex models are *less parsimonious* (more parameters need to be estimated). If a complex model can be replaced by a simpler model with an equally good fit, one would always **prefer the simpler model**. \n",
        "\n",
        "You can formally test two **nested** models (i.e. fitted to the same data, one model with one or more additional model terms) using the `anova` function. If the F test turns out significant, the **larger / complex** model (in our case, the model including the interaction term) is a **better** fit to the data, and should thus be preferred over the smaller / simpler model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpMruLOoJ7Jw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mod.large <- aov(Petal_length ~ Infection * Elevation, data = morph)\n",
        "mod.small <- aov(Petal_length ~ Infection + Elevation, data = morph)\n",
        "print(anova(mod.large,mod.small))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRjB4MIhchzH",
        "colab_type": "text"
      },
      "source": [
        "![Question](https://img.icons8.com/flat_round/64/000000/question-mark.png)\n",
        "* Would you choose `mod.large` or `mod.small`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpiTpuIfl3UP",
        "colab_type": "text"
      },
      "source": [
        "Also have a look at the `step` function that allows to identify an optimal model among a much more extensive range of possible models. It does so by adding *one* model term to (or remove one model term from) an initial model (`mod.small` in this case). All single term additions and drops that are within the `scope` are tested, and the model with the smallest [AIC](https://en.wikipedia.org/wiki/Akaike_information_criterion) value is kept. These steps are iterated until the AIC does not decrease anymore, and the final model is returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmXg-iA4l2le",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mod.best <- step(object = mod.small, \n",
        "                 scope = list(lower = Petal_length ~ 1, \n",
        "                              upper = Petal_length ~ Infection * Elevation + Elevation/Population), \n",
        "                 direction = \"both\")\n",
        "\n",
        "par(mfrow = c(2,2))\n",
        "plot(mod.best)\n",
        "par(mfrow = c(1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qz6hXi63aBC",
        "colab_type": "text"
      },
      "source": [
        "![Question](https://img.icons8.com/flat_round/64/000000/question-mark.png)\n",
        "* Which model is selected? (don't forget to check the residuals of this model as well!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j8dtR-v46qG",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "### Contrasts\n",
        "A [**contrast**](https://en.wikipedia.org/wiki/Contrast_(statistics)) is a specific hypothesis to be tested. Imagine that you are not only interested in an *overall* difference in `Petal_length` between `Population`s (or put differently: an overall effect of `Population` on `Petal_length`), but you want to test whether a *specific* population is different from another (or a specific treatment different from a control). You can think of them as t-tests carried out between specific factor levels. \n",
        "\n",
        "You can use **Tukey's Honest Significant Differences** method to test for differences in the response variable between *all* pairs of factor levels. This method is implemented in the `TukeyHSD` function but can be used as an option in the more flexible `glht` function from the [multcomp](https://cran.r-project.org/web/packages/multcomp/vignettes/multcomp-examples.pdf) package. The drawback here is that with many factor levels, you are carrying out a lot of tests, and not all of them answer your questions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQSmiWvl5EO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit a model of Petal_length and Population\n",
        "mod2 <- aov(Petal_length ~ Population, data = morph)\n",
        "\n",
        "# perform tests for all pairwise comparisons of populations\n",
        "tukey <- glht(mod2, linfct = mcp(Population = \"Tukey\"))\n",
        "# equivalent: tukey <- TukeyHSD(mod2, which = \"Population\")\n",
        "\n",
        "# tabular output\n",
        "summary(tukey)\n",
        "\n",
        "# graphical output\n",
        "oldmar <- par()$mar # get default margins\n",
        "par(mar = c(5.1, 12, 4.1, 2.1)) # give more left margin\n",
        "plot(tukey)\n",
        "par(mar = oldmar) # reset margins to default\n",
        "\n",
        "# residual analysis for mod2\n",
        "par(mfrow=c(2,2))\n",
        "plot(mod2)\n",
        "par(mfrow=c(2,2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyohcS5DRmnx",
        "colab_type": "text"
      },
      "source": [
        "![Question](https://img.icons8.com/flat_round/64/000000/question-mark.png)\n",
        "* Which population pair shows the largest difference in `Petal_length`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Wa5iGOic-Gp",
        "colab_type": "text"
      },
      "source": [
        "The `glht` function can do much more. It allows you to control which comparisons (tests) should be carried out. Each comparison is a separate test on the same dataset, therefore we have a **multiple testing problem**. The function `summary` on `glht` objects allows to set which p-value correction method should be used to correct for the multiple testing. The use of `glht` for specific contrasts requires the definition of a **contrast matrix**. It is a bit tricky to set up, but try to understand the following example, and you will be able to set specific contrasts that exactly match your research questions, while accounting for the multiple testing involved. These contrasts can be pairwise comparisons, but can also be more complex, as in the next example.\n",
        "\n",
        "Let us now test whether `Petal_length` is significatly different between \n",
        "- Zeneggen and (Faldumalp and Grengiols)\n",
        "- Zermatt and Niedergampel\n",
        "- Zermatt and Zeneggen\n",
        "- Zeneggen and Unterstalden\n",
        "- Zeneggen and Grengiols\n",
        "\n",
        "Follow these *rules* to create the **contrast matrix** *K*\n",
        "- use sensible rownames for each contrast (will appear in the summary)\n",
        "- columns correspond to the factor level (in the same order as `levels(<factor>)`, alphabetical by default)\n",
        "- set the coefficient to `0` if a level is not used in a contrast\n",
        "- use positive coefficients for levels in the first group and negative coefficients for levels in the second group\n",
        "- levels in the first and second group *must* add up to 1 and -1, respectively\n",
        "-Â each row *must* add up to zero"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Yom8z_Hc8bb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the contrast matrix for specific contrasts of interest\n",
        "#              first  -  second                 Fal, Gib, Gre, Nie, Sim, Unt, Zen, Zer\n",
        "K <- rbind(\"Zeneggen - Faldumalp+Grengiols\"= c(-1/2,   0,-1/2,   0,   0,   0,   1,   0),\n",
        "           \"Zermatt - Niedergampel\"        = c(   0,   0,   0,  -1,   0,   0,   0,   1),\n",
        "           \"Zermatt - Zeneggen\"            = c(   0,   0,   0,   0,   0,   0,  -1,   1),\n",
        "           \"Zeneggen - Unterstalden\"       = c(   0,   0,   0,   0,   0,  -1,   1,   0),\n",
        "           \"Zeneggen - Grengiols\"          = c(   0,   0,  -1,   0,   0,   0,   1,   0))\n",
        "\n",
        "# tabular output\n",
        "# \"single-step\" is the default p-value correcton method\n",
        "# The \"BH\" (Bonferroni-Holm) method is less conservative and more powerful than \"bonferroni\"\n",
        "summary(specific <- glht(mod2, linfct = mcp(Population = K)), test = adjusted(\"single-step\"))\n",
        "\n",
        "# corresponding t-tests for comparison\n",
        "t.test(morph$Petal_length[morph$Population %in% c(\"Zeneggen\")],\n",
        "       morph$Petal_length[morph$Population %in% c(\"Faldumalp\",\"Grengiols\")])\n",
        "t.test(morph$Petal_length[morph$Population %in% \"Zermatt\"],\n",
        "       morph$Petal_length[morph$Population %in% \"Niedergampel\"])\n",
        "t.test(morph$Petal_length[morph$Population %in% \"Zermatt\"],\n",
        "       morph$Petal_length[morph$Population %in% \"Zeneggen\"])\n",
        "t.test(morph$Petal_length[morph$Population %in% \"Zeneggen\"],\n",
        "       morph$Petal_length[morph$Population %in% \"Unterstalden\"])\n",
        "t.test(morph$Petal_length[morph$Population %in% \"Zeneggen\"],\n",
        "       morph$Petal_length[morph$Population %in% \"Grengiols\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbhGV_m5L-j0",
        "colab_type": "text"
      },
      "source": [
        "![Question](https://img.icons8.com/flat_round/64/000000/question-mark.png)\n",
        "Compare results of the specific contrasts to the TukeyHSD results and the t-test results. \n",
        "- Why is there a difference between the specific contrasts results and the TukeyHSD results?\n",
        "- Why is it better to specify specific contrasts rather than carry out multiple t-tests?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K_aVtDlBu1z",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "## Multivariate Analyses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJNrcM2gVtAH",
        "colab_type": "text"
      },
      "source": [
        "[Multivariate analysis](https://en.wikipedia.org/wiki/Multivariate_analysis) is a broad term that includes methods such as **Clustering analysis**, **Principal Component Analysis** (PCA), or **Linear Discriminant Analysis** (LDA).\n",
        "\n",
        "We will use a data set from my Master Thesis that contains morphological measurements on 10 leaf characters in eight [*Dalbergia*](https://en.wikipedia.org/wiki/Dalbergia) rosewood species from Madagascar. \n",
        "\n",
        "Let us read the data and get an overview. Note the use of `row.names` here: it automatically assigns row names of the resulting `data.frame` as the values stored in the column `ID`, and removes `ID` from the set of variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGdh_C4XBxGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d.mis <- read.csv(\"https://raw.githubusercontent.com/scrameri/Umweltbiologie/master/Multivariate.csv\", row.names = \"ID\")\n",
        "str(d.mis)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_VBrYbzV5Dl",
        "colab_type": "text"
      },
      "source": [
        "Since the dataset does not have too many variables, we can produce all pairwise plots using `ggparis` from the [*GGally*](https://cran.r-project.org/web/packages/GGally/index.html) package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoqP44DxPaZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gp <- ggpairs(data = d.mis)\n",
        "print(gp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba9Vzjy3jfGr",
        "colab_type": "text"
      },
      "source": [
        "You see that some of the variables are highly *correlated* among each other. While this is a potential problem for linear models, this is not a problem for many multivariate techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlBwoEGSRgwp",
        "colab_type": "text"
      },
      "source": [
        "###Â Data imputation\n",
        "Most multivariate techniques cannot handle missing values. Bear this in mind when designing experiments and measuring data, and make sure to collect data that is *as complete as possible*.\n",
        "\n",
        "One way to get around the missingness problem is to do a **Complete Case Analysis** by removing any individual with missing data. This may lead to loss of many individuals (most of the data) if missingness is distributed randomly accross individuals.\n",
        "\n",
        "In the case of large datasets such as genetic datasets, one often has little data for some variables (genetic loci) for technical reasons. These loci might however be highly correlated to other loci with less missingness, and can thus be entirely removed from analysis without much information loss. \n",
        "\n",
        "However, before you remove any variable with one or more missing values, it makes sense to remove individuals that have few overall data, because that might 'save' many variables from being excluded at the expense of just a few excluded individuals.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaK245jnWVo5",
        "colab_type": "text"
      },
      "source": [
        "Let us check how much missingness our dataset has, using a *self-written* **function** that computes the proportion of missing data.\n",
        "\n",
        "Think of `x` as a vector of numbers, e.g. one column or one row in the dataset. The missingness is a function of `x`, and computed as the `sum` of `NA` values in `x` divided by the number of observations in `x`. Such a function is only slightly more complex than standard functions such as `mean`, but it could be an arbitrarily complex function with many intermediate steps and variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btdOuM05Rw-2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the function that computes the percentage of missing values in a numeric vector\n",
        "missingness <- function(x) {sum(is.na(x))/length(x)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVrdldkYa7C4",
        "colab_type": "text"
      },
      "source": [
        "#### the apply function family\n",
        "Let us now apply this function to all columns, and all rows, to identify how much missingness these have. For such tasks, we use the `apply` familiy of functions. Have a look at the manuals for `apply`, `sapply`, `lapply` and `tapply`, these can be quite useful and save quite a lot of coding.\n",
        "\n",
        "The function `apply` takes a `data.frame` `X` as input, a `MARGIN` specification, and *applies* the function specified by the `FUN` argument to *each* element of the data subset defined by `MARGIN`. The output of `apply` can be a single value, a named vector or a matrix, depending on the `MARGIN` and `FUN` employed.\n",
        "\n",
        "`MARGIN` can be set as\n",
        "- `1` for \"apply function over each row\"\n",
        "- `2` for \"apply function over each column\"\n",
        "- `c(1,2)` for \"apply function over rows and columns\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV0BX5vWG-b8",
        "colab_type": "text"
      },
      "source": [
        "By the way: any further arguments that are taken by `FUN` can be passed as additional arguments, as indicated by the `...` in the `apply` function definition. If you wanted to apply the `mean` function to get column means, you could set `FUN = mean, na.rm = TRUE` to override the default behaviour that returns `NA` if there are missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PryfejCKhfDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(apply(X = d.mis[,-c(1:2)], MARGIN = 2, FUN = mean))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nE-FIVOkhsGN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(apply(X = d.mis[,-c(1:2)], MARGIN = 2, FUN = mean, na.rm = TRUE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heEyvTOJhwUl",
        "colab_type": "text"
      },
      "source": [
        "You can now combine `apply` with the self-written function `missingness` as follows (once over individuals, once over variables):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3WQf8JwbEIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mis.rows <- apply(X = d.mis[,-c(1:2)], MARGIN = 1, FUN = missingness) # MARGIN = 1 computes over *rows*\n",
        "mis.cols <- apply(X = d.mis[,-c(1:2)], MARGIN = 2, FUN = missingness) # MARGIN = 2 computes over *columns*\n",
        "\n",
        "# this sorts the individual and variable missingness and shows the 10 instances with highest missingness\n",
        "cat(\"\\nmissingness per individual\\n\")\n",
        "print(head(sort(mis.rows, decreasing = TRUE), n = 10))\n",
        "\n",
        "cat(\"\\nmissingness per variable\\n\")\n",
        "print(head(sort(mis.cols, decreasing = TRUE), n = 10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-Oeo6CvcoJd",
        "colab_type": "text"
      },
      "source": [
        "We see that one variable has nearly 10% missing data, while two individuals have 20% missingness. Excluding one out of 10 variables would throw away much more information than excluding two out of 126 individuals that likely contribute to the observed missingness in `leaflet.lower.hairy`. Before removing the two individuals, check what you are losing in terms of observations per `Species`.\n",
        "\n",
        "![Question](https://img.icons8.com/flat_round/64/000000/question-mark.png)\n",
        "- To which `Species` do the two individuals with high missingness belong?\n",
        "- Can we remove these individuals without losing data for a given `Species`? If yes, create a data.frame where these individuals are excluded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOLGzC4ae3Cn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqn5SYVHiSeW",
        "colab_type": "text"
      },
      "source": [
        "If the data table still contains missing values after removing individuals or/and variables with little data, these can be replaced by some unbiased values via **data imputation**. There are many sophisticated methods to do this. One simple method is to impute **variable means**.\n",
        "\n",
        "BEWARE: always bear in mind how much missingness your data has. Everitt & Hothorn (2011) write: \n",
        ">\"Imputing an observed variable mean for a variable with missing values preserves the observed sample means but distorts the covariance matrix [...], biasing estimated variances and covariances towards zero. [...] one should always bear in mind that the imputed values are not real measurements. We do not get something for nothing! And if there is a substantial proportion of individuals with large amounts of missing data, one should clearly question whether any form of statistical analysis is worth the bother.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXSaqlIGicQt",
        "colab_type": "text"
      },
      "source": [
        "![Question](https://img.icons8.com/flat_round/64/000000/question-mark.png)\n",
        "- Can you write a function that replaces all `NA` values in a vector by the `mean` of that vector? Hint: use the `return` function to return the imputed vector at the end of your function.\n",
        "- Can you apply that function to all variables with missing data, and assign the output to a `data.frame` named `d.imp`? Hint: apply returns a `matrix` rather than a `data.frame`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3FwVKxbiad1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your function and application here\n",
        "\n",
        "# Test your function on this test vector (yourfunction(tes))\n",
        "test <- c(1.1,3.2,6.4,NA,3.6)\n",
        "print(mean(test, na.rm = TRUE))\n",
        "\n",
        "# Apply your function to all numeric columns of d.mis, and assign the output to d.imp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gvejb-FaJfIc",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "###Â Data Aggregation\n",
        "It may be of interest to compute a **summary statistic** such as the `median` of *all* numeric variables *per species*. This would allow for a quick comparison between species.\n",
        "\n",
        "Of course, one could do subsets of the dataset for each species to calculate the summary statistic, or apply the `tapply` function for each variable, but that takes lines of code for each species or variable, respectively. There is an even more powerful function to do such calculations in *one* line of code. Meet the `aggregate` function: it takes a `data.frame` of numeric variables as input, and applies a specified function `FUN` to *subsets* of each variable. The subsets are defined by the `by` argument, and can include one or more factor variables, which *have* to be specified as a *list*.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8AZOEU-JrVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aggregate(x = d.mis[,-c(1,2)], by = list(d.mis$Species), FUN = median, na.rm = TRUE)\n",
        "\n",
        "# It even works if we want to summarize (aggregate) the data by two factors'\n",
        "aggregate(x = d.mis[,-c(1,2)], by = list(d.mis$Species, d.mis$Region), FUN = median, na.rm = TRUE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2Pccgb6MbBi",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "### Plot multivariate data\n",
        "There are a number of special plots designed to visualize large multivariate datasets. One of them is the **Co-Plot**. A Co-Plot displays scatterplots of two variables, but separately for different states of up to two other continuous or discrete variables.\n",
        "\n",
        "The `coplot` function takes the variables to be displayed as a formula:\n",
        "- `coplot(y ~ x | separated_by_x + separated_by_y, data = data)`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl6ue35LMmep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# color vector for 8 species\n",
        "col <- c(\"#A6CEE3\",\"#72B29C\",\"#4F9F3B\",\"#E93E3F\",\"#FDAC4F\",\"#D1AAB7\",\"#A99099\",\"#B15928\")\n",
        "\n",
        "# Co-Plot\n",
        "coplot(leaflet.shape1 ~ leaflet.length | Species + Region, data = d.mis, \n",
        "       col = col[d.mis$Species], pch = 19)  # note the use of col\n",
        "legend(\"topright\",levels(d.mis$Species),pch = 19, col = col, \n",
        "       xpd = TRUE, bty = \"n\", cex = 0.7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9oJnmmNN7ry",
        "colab_type": "text"
      },
      "source": [
        "Now we already see that Species1, 2, 3 and 5 are similar with respect to leaflet.length and leaflet.shape1, or that Species 6 has by far the longest leaflets. But what about overall similarity based on all the data (could be thousands of variables, think e.g. of next-generation sequencing data)? This is a **dimensionality problem**, which is where **Principal Components Analysis** (PCA) can help."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gtNGpDRzL2Y",
        "colab_type": "text"
      },
      "source": [
        "PCA can only handle **numeric** data (continuous or discrete). Let us therefore subset the numeric variables from the imputed dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOx1BRKhzRUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numericvars <- names(d.mis)[-c(1:2)]\n",
        "factorvars <- names(d.mis)[1:2]\n",
        "\n",
        "# define impute function\n",
        "impute <- function(x) {x[is.na(x)] <- mean(x, na.rm = TRUE) ; return(x)}\n",
        "\n",
        "# apply impute function and assign data.frame with factor variables\n",
        "d.imp <- data.frame(d.mis[,factorvars], apply(d.mis[,numericvars], 2, impute))\n",
        "\n",
        "# subset data.frame to include numeric variables only\n",
        "d.pca <- d.imp[,numericvars]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2VirwJE5JkC",
        "colab_type": "text"
      },
      "source": [
        "### Principal Component Analysis (PCA)\n",
        "\n",
        "Before PCA is carried out, the dataset is sometimes transformed. Two transformations are important here:\n",
        "- centering `x <- x-mean(x)`\n",
        "- scaling `x <- x/sd(x)`\n",
        "\n",
        "In PCA, all variables are usually **centered** around zero, by subtracting their mean. The question is whether one should **scale** the variables to unit variance, that is, divide them by their standard deviation. \n",
        "\n",
        "Rules: \n",
        "- if you deal with variables that measure different aspects on *different scales* such as size and shape, these variables will have very *different variance*. If you want to normalize for the different scale, it is very important to scale the variables to unit variance.\n",
        "- remove any variable with *zero* variance, otherwise prcomp will throw an error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKOuq3jb6oi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(sort(apply(X = d.pca, MARGIN = 2, FUN = var), decreasing = TRUE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBQi1HsuQnDa",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Every variable has variation, but `leaflet.area` has 10^10 times the variance of `leaflet.shape2`. That ist due to the scale of measurement, not due to the amount of information that these variables contain. We will therefore center and scale the dataset for PCA as follows:![alt text](https://)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llLij22B7Alz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Principal Component Analysis (PCA)\n",
        "pca <- prcomp(x = d.pca, center = TRUE, scale. = TRUE) # don't forget the \".\" after scale!\n",
        "# prcomp(x = scale(d.pca, center = TRUE, scale = TRUE), center = FALSE, scale. = FALSE)' # equivalent\n",
        "\n",
        "# Structure of objects of class 'pca'\n",
        "str(pca)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE_eAy5s7QBC",
        "colab_type": "text"
      },
      "source": [
        "In `pca` objects, there are three important results to look at:\n",
        "- `$x` gives the **PCA scores**, the coordinates of each individual in PCA space\n",
        "- `$sdev` contains information on the squared variance (**standard deviation**) of each principal component (PC)\n",
        "- `$rotation` contains **variable loadings**, the information on how much each original variable contributs to each PC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL5oYIBr9GEb",
        "colab_type": "text"
      },
      "source": [
        "#### The PCA scores\n",
        "Let us take a look at the PCA scores. Note why it was worth to set the `ID` variable as rownames."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozvf2Fjz9Qtx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "head(pca$x, n = 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMojDVWV9P0B",
        "colab_type": "text"
      },
      "source": [
        "If you have more individuals than variables, the PCA scores table has dimension [number of individuals, number of variables]. If you had less individuals than variables, the dimensions would be [number of individuals, number of individuals].\n",
        "  \n",
        "In total, the information contained in `pca$x` is equivalent to the information in the input data. The difference between the PCA scores and the original data is that the first few PCs contain most of the variation (i.e., information), whereas the informative variation may be scattered throughout all variables in the original data. This **reduction of dimensionality** is one important goal of PCA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9M01k2Z9XQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dim(pca$x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "resRPbTu9P2_",
        "colab_type": "text"
      },
      "source": [
        "#### The variance explained\n",
        "In PCA, we want to know the *fraction of variance* contained in (explained by) each principal component. Remember from your statistics course that variance is squared standard deviation. The fraction is simply computed as the component variance divided by the total variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tY7nBy8_kVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Explained variance\n",
        "# print(pca$sdev^2) # equivalent to apply(X = pca$x, MARGIN = 2, FUN = var)\n",
        "\n",
        "# Fraction of explained variance\n",
        "print(var <- pca$sdev^2 / sum(pca$sdev^2)) # magical formula to get PC variances\n",
        "\n",
        "# Assign names to the variance vector\n",
        "names(var) <- paste0(\"PC\", 1:length(var)) # makes a named vector\n",
        "\n",
        "# Cumulative fractions of variance should add up to 1\n",
        "print(cumsum(var))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuOAwROnAVQC",
        "colab_type": "text"
      },
      "source": [
        "Explained variance is often presented as a **Screeplot**, which is just a barplot of component variances:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovlqtfPiA0Mb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This plot helps to see which PCs contain a lot of information, i.e. are biologically meaningful.\n",
        "barplot(100*var, xlab = \"Principal Component\", ylab = \"Component Variance (%)\")\n",
        "\n",
        "# This plot helps to decide which PCs explain enough variance to satisfy a certain criterion, e.g. 80% of total variance.\n",
        "barplot(100*cumsum(var), xlab = \"Principal Component\", ylab = \"Cumulative Component Variance (%)\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80n0nizlBJmB",
        "colab_type": "text"
      },
      "source": [
        "#### Variable loadings\n",
        "\n",
        "The variable loadings are the contribution of each original variable to each Principal Component. These are stored in the rotation matrix `$rotation`. I've written a function that vizualizes the relative importance of each variable to each principal component."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfdnXVRvBc0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loadings <- data.frame(pca$rotation)\n",
        "\n",
        "loadingplot <- function(x, component = 1, abs = TRUE) {\n",
        "  # take absolute values if indicated\n",
        "  if (abs) {x <- abs(x)}\n",
        "\n",
        "  # plot\n",
        "  c <- x[,component]\n",
        "\n",
        "  ggplot(x, aes(x = seq_along(c), xend = seq_along(c),\n",
        "                y = 0, yend = c)) +\n",
        "    geom_segment() +\n",
        "    geom_text(aes(x = seq_along(c), y = c, \n",
        "                  label = rownames(x)), size = 3) +\n",
        "    scale_x_continuous(breaks = seq_along(c), \n",
        "                     labels = seq_along(c)) +\n",
        "    labs(x = \"PC Component\", \n",
        "         y = paste(ifelse(abs, \"Absolute \", \"\"), \"PC Loading (Coefficient)\")) +\n",
        "    ggtitle(paste(\"Component\", component)) +\n",
        "    theme_bw()\n",
        "}\n",
        "\n",
        "loadingplot(loadings, 1, abs = TRUE)\n",
        "loadingplot(loadings, 2, abs = TRUE)\n",
        "loadingplot(loadings, 3, abs = TRUE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19uwFVkvI6dB",
        "colab_type": "text"
      },
      "source": [
        "![Question](https://img.icons8.com/flat_round/64/000000/question-mark.png)\n",
        "- What do the first three components 'measure'?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYxSgeGZgVmY",
        "colab_type": "text"
      },
      "source": [
        "#### The relationship between variable loadings and variable scores\n",
        "You can think of the PC variable loadings as the **coefficients** that fit the input data to a PC score. In other words: \n",
        "> the PC axes are the *linear combination* of the *data vectors* (rows or individuals) with the loadings as coefficients.\n",
        "\n",
        "The larger the absolute coefficient, the larger the contribution of that variable to the principal component. If a large coefficient is positive, individuals with high values in that variable will have a positive PC axis score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dTw0KE8iSfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the input to PCA (the scaled and centered variables)\n",
        "pca.input <- as.matrix(scale(d.pca, center = TRUE, scale = TRUE))\n",
        "\n",
        "# from the rotation matrix, calculate the coordinate of SH031 on PC1\n",
        "cat(\"input data\\n\")\n",
        "print(pca.input[\"SH156\",])\n",
        "cat(\"\\nPC 1 loadings\\n\")\n",
        "print(pca$rotation[,1])\n",
        "\n",
        "# calculate by hand\n",
        "cat(\"\\nby hand:\\n\")\n",
        "print(1.606827*0.3319507 + 2.988947*0.3742269 + 3.849139*0.3881035 + 4.332554*0.3659823 + -1.118697*-0.2518057 + -1.703997*-0.3317078 + -1.526139*-0.3272454 + -2.532783*-0.3065936 + 1.356265*0.1160678 + -2.155491*-0.2778463)\n",
        "\n",
        "# calculate one PC score by scalar multiplication\n",
        "cat(\"\\nby scalar multiplication:\\n\")\n",
        "print(pca.input[\"SH156\",] %*% pca$rotation[,1])\n",
        "\n",
        "# calculate all PC scores by matrix multiplication\n",
        "cat(\"\\nidentity test:\\n\")\n",
        "scores <- pca.input %*% pca$rotation\n",
        "print(all.equal(scores, pca$x)) # if TRUE, you did it!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j6hBd5_nSGz",
        "colab_type": "text"
      },
      "source": [
        "#### Mathematics of PC loadings\n",
        "**Component variances** in a PCA on some data are the same as the **eigenvalues** of the **covariance matrix** of the data.\n",
        "\n",
        "If PCA is carried out on the scaled data, the covariance matrix is transformed to the **correlation matrix** before PCA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbzE4ixWgcHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# eigenvalues\n",
        "cat(\"Standard deviations of principal components\\n\")\n",
        "print(pca$sdev^2)\n",
        "\n",
        "cat(\"Eigenvalues of the scaled covariance matrix\\n\")\n",
        "print(eigen(cov(scale(d.pca)))$value)\n",
        "\n",
        "cat(\"Eigenvalues of the correlation matrix\\n\")\n",
        "print(eigen(cor(d.pca))$value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2G47X6thhtl",
        "colab_type": "text"
      },
      "source": [
        "**PCA loadings** (rotation matrix) on a PCA of some data are the same as the **eigenvectors** of the covariance matrix of the data (sign can be opposite)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbqz9pmyovZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# eigenvectors\n",
        "cat(\"Principal component loadings\\n\")\n",
        "print(head(pca$rotation, n = 3))\n",
        "\n",
        "cat(\"\\nEigenvectors of the scaled covariance matrix\\n\")\n",
        "print(head(eigen(cov(scale(d.pca)))$vectors, n = 3))\n",
        "\n",
        "cat(\"\\nEigenvectors of the correlation matrix\\n\")\n",
        "print(head(eigen(cor(d.pca))$vectors, n = 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IElndVnFMY5b",
        "colab_type": "text"
      },
      "source": [
        "#### Visualizing PCA results\n",
        "Finally, we want to see the famous scatterplot of the first PCA components, nicely colored by some covariates. The `factoextra` package offers handy functions to plot both the PCA loadings and the PCA scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTGIMx-SMrGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## PCA variable loadings (here displayed for PC 1 and 2)\n",
        "fviz_pca_var(pca, col.var = \"contrib\", axes = c(1,2),\n",
        "             gradient.cols = c(\"gray25\", \"gray45\", \"red\"), repel = TRUE)\n",
        "\n",
        "## PCA scores (here for PC 1 and 2, with 90% confidence ellipses around Species)\n",
        "fviz_pca_ind(pca, axes = c(1, 2),\n",
        "             habillage = d.imp$Species, # habillage is the factor used for colors\n",
        "             palette = col,\n",
        "             addEllipses = TRUE, ellipse.level = 0.9, # sets size of ellipses\n",
        "             ellipse.alpha = 0.5,  # sets ellipse transparency \n",
        "             label = \"ind\", labelsize = 1, # use label = \"none\" to omit labels\n",
        "             repel = TRUE) +\n",
        "  coord_fixed() # sets the same scale for both components"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSAP1-Mxd_Hy",
        "colab_type": "text"
      },
      "source": [
        "![Question](https://img.icons8.com/flat_round/64/000000/question-mark.png)\n",
        "By looking at these visualizations only:\n",
        "- Which variables are highly correlated? Verify using `cor` or `cor.test`.\n",
        "- Which `Species` has the longest leaves and largest leaflets?\n",
        "- Which `Species` have hairy leaves and leaflets?\n",
        "\n",
        "![Question](https://img.icons8.com/flat_round/64/000000/question-mark.png)\n",
        "Carry out a nested PC analysis within the morphologically similar species 1, 3, 4 and 5, and dispaly the results.\n",
        "- Can these species be differentiated based on these leaf characters? If so, by which variables?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ptg7SHMveXub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# subset\n",
        "d.pca2 <- d.pca[d.imp$Species %in% c(\"Species1\",\"Species2\",\"Species3\",\"Species5\"),]\n",
        "d.imp2 <- d.imp[d.imp$Species %in% c(\"Species1\",\"Species2\",\"Species3\",\"Species5\"),]\n",
        "\n",
        "# PCA\n",
        "pca2 <- prcomp(d.pca2, center = TRUE, scale. = TRUE)\n",
        "\n",
        "# PCA screeplot\n",
        "var2 <- pca2$sdev^2/sum(pca2$sdev^2) \n",
        "barplot(100*var2, xlab = \"Principal Component\", ylab = \"Component Variance (%)\")\n",
        "\n",
        "# PCA loadings\n",
        "fviz_pca_var(pca2, col.var = \"contrib\", axes = c(1,2),\n",
        "             gradient.cols = c(\"gray25\", \"gray45\", \"red\"), repel = TRUE)\n",
        "\n",
        "# PCA scores of PC1 and PC2\n",
        "fviz_pca_ind(pca2, axes = c(1, 2),\n",
        "             habillage = d.imp2$Species, # habillage is the factor used for colors\n",
        "             palette = col,\n",
        "             addEllipses = TRUE, ellipse.level = 0.9, # sets size of ellipses\n",
        "             ellipse.alpha = 0.5,  # sets ellipse transparency \n",
        "             label = \"ind\", labelsize = 1, # use label = \"none\" to omit labels\n",
        "             repel = TRUE) +\n",
        "  coord_fixed() # sets the same scale for both components"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHZ9kqpvfbkn",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "## References\n",
        "- G. W. Oehlert (2000) A first course in design and analysis of experiments. Freeman & Co.\n",
        "- B. Everitt & T. Hothorn (2011) An Introduction to Applied Multivariate Analysis with R. Springer.\n",
        "\n",
        "### Links\n",
        "ANOVA Teaching Notes by Dr. Lukas Meier, Senior Scientist at the Seminar for Statistics\n",
        "- https://stat.ethz.ch/~meier/teaching/anova/"
      ]
    }
  ]
}